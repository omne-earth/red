#!/bin/bash
set -ueox pipefail

# prerequisites - nvidia drivers, cuda toolkit, nvidia container toolkit

# enter the toolbox container
sudo dnf install toolbox
toolbox create --image registry.fedoraproject.org/fedora-toolbox:41 --container fedora-toolbox-41-cuda
toolbox enter --container fedora-toolbox-41-cuda

# install development tools
sudo dnf distro-sync
sudo dnf install vim-default-editor --allowerasing
sudo dnf install @c-development @development-tools cmake

# sideload cuda from host to the toolbox/guest
sudo dnf config-manager addrepo --from-repofile=https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.
sudo dnf distro-sync
ls -la /usr/lib64/libcuda.so.1
sudo dnf download --destdir=/tmp/nvidia-driver-libs --resolve --arch x86_64 nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced
sudo rpm --install --verbose --hash --justdb /tmp/nvidia-driver-libs/*

# check if sideload okay, all packages should be "is already installed"
sudo dnf install nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced

# install cuda in the toolbox
sudo dnf install cuda

>>>>>>>>>>>> build
cmake -B build -DGGML_CUDA=ON 
# cmake -B build -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=OFF
cmake --build build --config Release -j 12

# https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#runtime-cuda-environmental-variables

# Unified memory - https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#unified-memory
GGML_CUDA_ENABLE_UNIFIED_MEMORY=1

# Performance Tunin - https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#unified-memory
Option	Legal-values	Default	Description

GGML_CUDA_FORCE_MMQ	
Boolean	
false	
Force the use of custom matrix multiplication kernels for quantized models instead of FP16 cuBLAS even if there is no int8 tensor core implementation available (affects V100, CDNA and RDNA3+). MMQ kernels are enabled by default on GPUs with int8 tensor core support. With MMQ force enabled, speed for large batch sizes will be worse but VRAM consumption will be lower.

GGML_CUDA_FORCE_CUBLAS
Boolean
false
Force the use of FP16 cuBLAS instead of custom matrix multiplication kernels for quantized models

GGML_CUDA_F16
Boolean
false
If enabled, use half-precision floating point arithmetic for the CUDA dequantization + mul mat vec kernels and for the q4_1 and q5_1 matrix matrix multiplication kernels. Can improve performance on relatively recent GPUs.

GGML_CUDA_PEER_MAX_BATCH_SIZE
Positive integer
128
Maximum batch size for which to enable peer access between multiple GPUs. Peer access requires either Linux or NVLink. When using NVLink enabling peer access for larger batch sizes is potentially beneficial.

GGML_CUDA_FA_ALL_QUANTS
Boolean
false
Compile support for all KV cache quantization type (combinations) for the FlashAttention CUDA kernels. More fine-grained control over KV cache size but compilation takes much longer.


sudo sh -c 'echo "export PATH=\$PATH:/usr/local/cuda/bin" >> /etc/profile.d/cuda.sh'
sudo chmod +x /etc/profile.d/cuda.sh
source /etc/profile.d/cuda.sh

# huggingface login
git config --global credential.helper store
sudo dnf install huggingface-cli
huggingface-cli login

sudo dnf instlal git-lfs
git clone https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf
git clone https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf
git clone https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf
git clone https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf

cd /home/shree/workspace/red/llama.cpp/build/bin
./build/bin/llama-cli -m /home/shree/workspace/red/models/gemma-3-4b-it-qat-q4_0-gguf/gemma-3-4b-it-q4_0.gguf
GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 ./build/bin/llama-cli -m /home/shree/workspace/red/models/gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf
GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 ./build/bin/llama-cli -m /home/shree/workspace/red/models/gemma-3-1b-it-qat-q4_0-gguf/gemma-3-1b-it-q4_0.gguf

# next
# compile cpu only with blas and blis
# cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
# https://github.com/ggml-org/llama.cpp/blob/916c83bfe7f8b08ada609c3b8e583cf5301e594b/docs/backend/BLIS.md